# Bandit-Bench

A comprehensive benchmarking framework for comparing contextual bandit algorithms across multiple libraries.

## Overview

Bandit-Bench provides a unified interface for evaluating and comparing contextual bandit implementations from popular Python libraries including:
- **Vowpal Wabbit** - Fast, production-ready contextual bandits
- **PyTorch Bandits** - Deep learning-based bandit algorithms
- **River** - Online machine learning bandits
- **Contextual Bandits Library** - Traditional bandit implementations

## Features

- ðŸŽ¯ **Unified API** - Common interface across different bandit libraries
- ðŸ“Š **Comprehensive Metrics** - Regret, cumulative reward, and performance tracking
- ðŸ“ˆ **Visualization** - Interactive dashboards and comparison plots
- ðŸ”„ **Simulation Engine** - Robust framework for running experiments
- ðŸ§ª **Extensible** - Easy to add new algorithms and datasets
- ðŸ“¦ **Dataset Management** - Built-in dataset handling and generation

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd bandit-bench
```

2. Install dependencies using `uv` (recommended):
```bash
uv sync
```

Or using pip:
```bash
pip install -e .
```

## Quick Start

Run a comparison experiment:

```bash
python -m cb_comparison.cli
```

Or use the Makefile:

```bash
make run
```

## Project Structure

```
bandit-bench/
â”œâ”€â”€ src/cb_comparison/          # Main source code
â”‚   â”œâ”€â”€ bandits/                # Bandit implementations
â”‚   â”‚   â”œâ”€â”€ cb_library.py       # Contextual Bandits Library wrapper
â”‚   â”‚   â”œâ”€â”€ pytorch_bandit.py   # PyTorch bandits wrapper
â”‚   â”‚   â”œâ”€â”€ river_bandit.py     # River bandits wrapper
â”‚   â”‚   â””â”€â”€ vowpal_bandit.py    # Vowpal Wabbit wrapper
â”‚   â”œâ”€â”€ data/                   # Dataset management
â”‚   â”œâ”€â”€ evaluation/             # Metrics and comparison tools
â”‚   â””â”€â”€ utils/                  # Visualization utilities
â”œâ”€â”€ experiments/results/        # Experiment outputs
â”œâ”€â”€ tests/                      # Unit tests
â””â”€â”€ pyproject.toml             # Project configuration
```

## Usage

### Running Experiments

```python
from cb_comparison.evaluation.comparison import run_comparison
from cb_comparison.data.dataset import load_dataset

# Load dataset
dataset = load_dataset("my_dataset")

# Run comparison
results = run_comparison(
    dataset=dataset,
    algorithms=["vowpal", "pytorch", "river"],
    n_rounds=1000
)
```

### Visualizing Results

```python
from cb_comparison.utils.visualization import plot_regret_curves

# Generate plots
plot_regret_curves("experiments/results/regret_curves.csv")
```

## Development

Run tests:
```bash
make test
```

Run linting:
```bash
make lint
```

Format code:
```bash
make format
```

## Results

Experiment results are saved in `experiments/results/` including:
- CSV files with detailed metrics
- PNG plots comparing algorithm performance
- Interactive HTML dashboards

## Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## License

See `LICENSE` file for details.

## Requirements

- Python 3.10+
- Dependencies managed via `uv` and `pyproject.toml`

## Contact

For issues and questions, please open an issue on GitHub.